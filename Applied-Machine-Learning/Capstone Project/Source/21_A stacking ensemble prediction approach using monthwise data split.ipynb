{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble Prediction Approach\n",
    "\n",
    "Finally after the feature engineering, data formatting and individual model analysis, creating  a Stacking ensemble model using the below models.\n",
    "    - Linear Regression\n",
    "    - Negative Binomial\n",
    "    - Random Forest\n",
    "    - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # fundamental package for scientific computing\n",
    "import pandas as pd   # Python Data Analysis Library\n",
    "from pandas import Series # one-dimensional labeled array capable of holding any data type \n",
    "import seaborn as sns # library for making statistical graphics in Python\n",
    "import os             # operating system dependent functionality, file descriptor..\n",
    "\n",
    "from sklearn.model_selection import train_test_split             # Split arrays or matrices into random train and test subsets\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error                  # Mean absolute error regression loss\n",
    "from sklearn.metrics import median_absolute_error                # Median absolute error regression loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                                # For warning control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotly library for visalization\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "%matplotlib inline    \n",
    "# Line magic command will make plot outputs appear and be stored within the notebook.\n",
    "import matplotlib.pyplot as plt   # matplotlib's plotting framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use these 4 base models for the stacking\n",
    "from sklearn.linear_model import LinearRegression   # base model for linear regression\n",
    "from sklearn.linear_model import Lasso              # least absolute shrinkage and selection operator\n",
    "from sklearn.linear_model import Ridge              # ridge regression\n",
    "import statsmodels.api as sm                        # Negative binomial regression\n",
    "import statsmodels.formula.api as smf               # statistical models formula APIs\n",
    "from sklearn.ensemble import RandomForestRegressor  # A random forest regressor\n",
    "import xgboost as xgb                               # Gradient Boosting for regression\n",
    "\n",
    "from sklearn.model_selection import KFold           # K-Folds cross-validator \n",
    "from sklearn.model_selection import GridSearchCV    # Grid Search for paramter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dengue_test_iq.pkl',\n",
       " 'dengue_test_sj.pkl',\n",
       " 'dengue_train_iq.pkl',\n",
       " 'dengue_train_sj.pkl',\n",
       " 'test_iq_month_1.pkl',\n",
       " 'test_iq_month_10.pkl',\n",
       " 'test_iq_month_11.pkl',\n",
       " 'test_iq_month_12.pkl',\n",
       " 'test_iq_month_2.pkl',\n",
       " 'test_iq_month_3.pkl',\n",
       " 'test_iq_month_4.pkl',\n",
       " 'test_iq_month_5.pkl',\n",
       " 'test_iq_month_6.pkl',\n",
       " 'test_iq_month_7.pkl',\n",
       " 'test_iq_month_8.pkl',\n",
       " 'test_iq_month_9.pkl',\n",
       " 'test_sj_month_1.pkl',\n",
       " 'test_sj_month_10.pkl',\n",
       " 'test_sj_month_11.pkl',\n",
       " 'test_sj_month_12.pkl',\n",
       " 'test_sj_month_2.pkl',\n",
       " 'test_sj_month_3.pkl',\n",
       " 'test_sj_month_4.pkl',\n",
       " 'test_sj_month_5.pkl',\n",
       " 'test_sj_month_6.pkl',\n",
       " 'test_sj_month_7.pkl',\n",
       " 'test_sj_month_8.pkl',\n",
       " 'test_sj_month_9.pkl',\n",
       " 'train_iq_month_1.pkl',\n",
       " 'train_iq_month_10.pkl',\n",
       " 'train_iq_month_11.pkl',\n",
       " 'train_iq_month_12.pkl',\n",
       " 'train_iq_month_2.pkl',\n",
       " 'train_iq_month_3.pkl',\n",
       " 'train_iq_month_4.pkl',\n",
       " 'train_iq_month_5.pkl',\n",
       " 'train_iq_month_6.pkl',\n",
       " 'train_iq_month_7.pkl',\n",
       " 'train_iq_month_8.pkl',\n",
       " 'train_iq_month_9.pkl',\n",
       " 'train_sj_month_1.pkl',\n",
       " 'train_sj_month_10.pkl',\n",
       " 'train_sj_month_11.pkl',\n",
       " 'train_sj_month_12.pkl',\n",
       " 'train_sj_month_2.pkl',\n",
       " 'train_sj_month_3.pkl',\n",
       " 'train_sj_month_4.pkl',\n",
       " 'train_sj_month_5.pkl',\n",
       " 'train_sj_month_6.pkl',\n",
       " 'train_sj_month_7.pkl',\n",
       " 'train_sj_month_8.pkl',\n",
       " 'train_sj_month_9.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check what data files are available.\n",
    "PATH=\"./../datasets/\"\n",
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the train and test data\n",
    "train_filename_sj = ( './../datasets/dengue_train_sj.pkl' )\n",
    "train_filename_iq = ( './../datasets/dengue_train_iq.pkl' )\n",
    "test_filename_sj  = ( './../datasets/dengue_test_sj.pkl' )\n",
    "test_filename_iq  = ( './../datasets/dengue_test_iq.pkl' )\n",
    "\n",
    "dengue_train_sj = pd.read_pickle( train_filename_sj )\n",
    "dengue_train_iq = pd.read_pickle( train_filename_iq )\n",
    "dengue_test_sj  = pd.read_pickle( test_filename_sj )\n",
    "dengue_test_iq  = pd.read_pickle( test_filename_iq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, reg, seed=0, params=None, NB=False,col=[],alpha=0.001):\n",
    "        self.NB = NB\n",
    "        self.col = col\n",
    "        self.alpha = alpha\n",
    "        if( self.NB == True ):\n",
    "            self.model = None\n",
    "        else:\n",
    "            self.model = reg(**params)\n",
    "            params['random_state'] = seed\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        if( self.NB == False ):\n",
    "            self.model.fit(x_train, y_train)\n",
    "        else:\n",
    "            formula = ' + '.join([ str(feature) for feature in list(self.col)])\n",
    "            formula = 'y ~ ' + formula\n",
    "            train      = pd.DataFrame( x_train )\n",
    "            train['y'] = y_train\n",
    "            self.model = smf.glm( formula = formula,\n",
    "                                  data = train,\n",
    "                                  family = sm.families.NegativeBinomial(alpha=self.alpha)).fit()\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.model.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        if( self.NB == False ):\n",
    "            print( self.model.fit(x,y).feature_importances_ )\n",
    "        else:\n",
    "            print( self.model.feature_importances_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f67620d-b531-a2fa-c297-e951970c3c28",
    "_uuid": "4d193c581df258e823aff2796bf015cf906aac99"
   },
   "source": [
    "### Out-of-Fold Predictions\n",
    "\n",
    "Stacking uses predictions of base regressor as input for training to a second-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "406d0494-1d0c-3126-19d9-bc53127c4249",
    "_uuid": "46a93dc062e973832cecd50246d0d7581aafb02b"
   },
   "outputs": [],
   "source": [
    "def get_OutOfFoldPredictions( model, x_train, y_train, x_test ):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tr = x_train.iloc[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train.iloc[test_index]\n",
    "        model.train(x_tr, y_tr)\n",
    "        oof_train[test_index] = model.predict(x_te)\n",
    "        oof_test_skf[i, :] = model.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cd92196-f7ba-4f14-0fc4-36520fbcb2ca",
    "_uuid": "7b1a7767ae61b6b217a3311e89190b05ab0a4891"
   },
   "source": [
    "# Generating Base First-Level Models \n",
    "\n",
    "So now let us prepare four learning models as our first level regression.\n",
    "- The models are listed as follows:\n",
    "    - Linear Regression\n",
    "    - Negative Binomial\n",
    "    - Random Forest\n",
    "    - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params_sj = {\n",
    "    'n_estimators': 220,\n",
    "    'max_depth': 4,\n",
    "    'criterion': 'mae',\n",
    "    'min_samples_split': 10,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'verbose': 0\n",
    "}\n",
    "        \n",
    "rf_params_iq = {\n",
    "    'n_estimators': 22,\n",
    "    'max_depth': 4,\n",
    "    'criterion': 'mae',\n",
    "    'min_samples_split': 16,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params_sj = {\n",
    "    'learning_rate': 0.01, \n",
    "    'n_estimators': 225,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 12,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.95,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 1e-5,\n",
    "    'objective':'reg:squarederror',\n",
    "    'verbose': 0\n",
    "}\n",
    "xgb_params_iq = {\n",
    "    'learning_rate': 0.1, \n",
    "    'n_estimators': 17,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.01,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0.75,\n",
    "    'reg_lambda': 0.75,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Linear Regression parameters ( L1 lasso regularization )\n",
    "l1_params_sj = {\n",
    "    'alpha': 2\n",
    "}\n",
    "lr_params_iq = {\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RFE selected features from Linear Regression\n",
    "col_RFE_sj = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw', 'reanalysis_air_temp_c',\n",
    "       'reanalysis_avg_temp_c', 'reanalysis_dew_point_temp_c',\n",
    "       'reanalysis_relative_humidity_percent',\n",
    "       'reanalysis_specific_humidity_g_per_kg', 'ndvi_mean',\n",
    "       'station_avg_temp_c_lagVar', 'reanalysis_max_air_temp_c_lagSum',\n",
    "       'reanalysis_air_temp_c_lagSum', 'reanalysis_air_temp_c_lagMean',\n",
    "       'reanalysis_air_temp_c_lagVar', 'reanalysis_avg_temp_c_lagSum',\n",
    "       'reanalysis_avg_temp_c_lagMean', 'reanalysis_avg_temp_c_lagVar',\n",
    "       'ndvi_ne_lagSum', 'ndvi_ne_lagMean', 'ndvi_ne_lagVar',\n",
    "       'reanalysis_tdtr_c_lagSum', 'reanalysis_tdtr_c_lagVar',\n",
    "       'ndvi_mean_lagVar', 'ndvi_nw_lagSum', 'ndvi_nw_lagVar',\n",
    "       'ndvi_se_lagSum', 'ndvi_se_lagMean', 'ndvi_se_lagVar', 'ndvi_sw_lagSum',\n",
    "       'ndvi_sw_lagMean', 'ndvi_sw_lagVar']\n",
    "col_RFE_iq = ['ndvi_nw', 'ndvi_sw', 'reanalysis_dew_point_temp_c',\n",
    "       'reanalysis_specific_humidity_g_per_kg',\n",
    "       'reanalysis_specific_humidity_g_per_kg_lagVar',\n",
    "       'reanalysis_dew_point_temp_c_lagVar', 'ndvi_se_lagVar',\n",
    "       'ndvi_sw_lagVar', 'ndvi_ne_lagVar', 'ndvi_nw_lagVar']\n",
    "col_FFS_iq = ['reanalysis_tdtr_c_lagVar', 'reanalysis_specific_humidity_g_per_kg_lagMean',\n",
    "          'reanalysis_precip_amt_kg_per_m2_lagVar', 'ndvi_se_lagVar', 'ndvi_ne_lagVar',\n",
    "          'reanalysis_precip_amt_kg_per_m2_lagMean', 'station_min_temp_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED   = 1 # for reproducibility\n",
    "NFOLDS = 10 # set folds for out-of-fold prediction\n",
    "kf = KFold( n_splits = NFOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objects that represent our models\n",
    "\n",
    "# For san Juan\n",
    "rf_sj = SklearnHelper(reg=RandomForestRegressor, seed=SEED, params=rf_params_sj)\n",
    "gb_sj = SklearnHelper(reg=xgb.XGBRegressor, seed=SEED, params=xgb_params_sj)\n",
    "nb_sj = SklearnHelper(reg=None, seed=SEED, params=None, NB=True, col=col_RFE_sj, alpha=1.18 )\n",
    "lr_sj = SklearnHelper(reg=Lasso, seed=SEED, params=l1_params_sj )\n",
    "\n",
    "# For Iquitos\n",
    "rf_iq = SklearnHelper(reg=RandomForestRegressor, seed=SEED, params=rf_params_iq)\n",
    "gb_iq = SklearnHelper(reg=xgb.XGBRegressor, seed=SEED, params=xgb_params_iq)\n",
    "nb_iq = SklearnHelper(reg=None, seed=SEED, params=None, NB=True, col=col_RFE_iq, alpha=0.001 )\n",
    "lr_iq = SklearnHelper(reg=LinearRegression, seed=SEED, params=lr_params_iq )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_oof_train_sj = np.array([])\n",
    "rf_oof_test_sj  = np.array([])\n",
    "gb_oof_train_sj = np.array([])\n",
    "gb_oof_test_sj  = np.array([])\n",
    "nb_oof_train_sj = np.array([])\n",
    "nb_oof_test_sj  = np.array([])\n",
    "lr_oof_train_sj = np.array([])\n",
    "lr_oof_test_sj  = np.array([])\n",
    "y_train_sj      = np.array([])\n",
    "for month in range( 1, 13 ):\n",
    "    train_filename = ( './../datasets/train_sj_month_' + str( month ) + '.pkl' )\n",
    "    test_filename  = ( './../datasets/test_sj_month_' + str( month ) + '.pkl' )\n",
    "    dengue_train_sj_month = pd.read_pickle( train_filename )\n",
    "    dengue_test_sj_month  = pd.read_pickle( test_filename )\n",
    "    x_train_sj = pd.DataFrame( dengue_train_sj_month )\n",
    "    x_train_sj.drop( columns = ['city','year','month','total_cases'], inplace = True )\n",
    "    y_train_sj_m = dengue_train_sj_month.total_cases\n",
    "    x_test_sj = pd.DataFrame( dengue_test_sj_month, columns = x_train_sj.columns )\n",
    "    ntrain = x_train_sj.shape[0]\n",
    "    ntest = x_test_sj.shape[0]\n",
    "    rf_oof_train_sj_m, rf_oof_test_sj_m = get_OutOfFoldPredictions( rf_sj, x_train_sj, y_train_sj_m, x_test_sj ) # Random Forest\n",
    "    gb_oof_train_sj_m, gb_oof_test_sj_m = get_OutOfFoldPredictions( gb_sj, x_train_sj, y_train_sj_m, x_test_sj ) # Gradient Boost\n",
    "    x_train_RFE_sj = x_train_sj[col_RFE_sj]\n",
    "    x_test_RFE_sj  = x_test_sj[col_RFE_sj]\n",
    "    nb_oof_train_sj_m, nb_oof_test_sj_m = get_OutOfFoldPredictions( nb_sj, x_train_RFE_sj, y_train_sj_m, x_test_RFE_sj )  # Negative Binomial\n",
    "    lr_oof_train_sj_m, lr_oof_test_sj_m = get_OutOfFoldPredictions( lr_sj, x_train_RFE_sj,y_train_sj_m, x_test_RFE_sj )  # Linear Regression\n",
    "    rf_oof_train_sj = np.append( rf_oof_train_sj, rf_oof_train_sj_m)\n",
    "    rf_oof_test_sj  = np.append( rf_oof_test_sj , rf_oof_test_sj_m )\n",
    "    gb_oof_train_sj = np.append( gb_oof_train_sj, gb_oof_train_sj_m)\n",
    "    gb_oof_test_sj  = np.append( gb_oof_test_sj , gb_oof_test_sj_m )\n",
    "    nb_oof_train_sj = np.append( nb_oof_train_sj, nb_oof_train_sj_m)\n",
    "    nb_oof_test_sj  = np.append( nb_oof_test_sj , nb_oof_test_sj_m )\n",
    "    lr_oof_train_sj = np.append( lr_oof_train_sj, lr_oof_train_sj_m)\n",
    "    lr_oof_test_sj  = np.append( lr_oof_test_sj , lr_oof_test_sj_m)\n",
    "    y_train_sj = np.append( y_train_sj , y_train_sj_m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_oof_train_iq = np.array([])\n",
    "rf_oof_test_iq  = np.array([])\n",
    "gb_oof_train_iq = np.array([])\n",
    "gb_oof_test_iq  = np.array([])\n",
    "nb_oof_train_iq = np.array([])\n",
    "nb_oof_test_iq  = np.array([])\n",
    "lr_oof_train_iq = np.array([])\n",
    "lr_oof_test_iq  = np.array([])\n",
    "y_train_iq      = np.array([])\n",
    "for month in range( 1, 13 ):\n",
    "    train_filename = ( './../datasets/train_iq_month_' + str( month ) + '.pkl' )\n",
    "    test_filename  = ( './../datasets/test_iq_month_' + str( month ) + '.pkl' )\n",
    "    dengue_train_iq_month = pd.read_pickle( train_filename )\n",
    "    dengue_test_iq_month  = pd.read_pickle( test_filename )\n",
    "    x_train_iq = pd.DataFrame( dengue_train_iq_month )\n",
    "    x_train_iq.drop( columns = ['city','year','month','total_cases'], inplace = True )\n",
    "    y_train_iq_m = dengue_train_iq_month.total_cases\n",
    "    x_test_iq = pd.DataFrame( dengue_test_iq_month, columns = x_train_iq.columns )\n",
    "    ntrain = x_train_iq.shape[0]\n",
    "    ntest = x_test_iq.shape[0]\n",
    "    rf_oof_train_iq_m, rf_oof_test_iq_m = get_OutOfFoldPredictions( rf_iq, x_train_iq, y_train_iq_m, x_test_iq ) # Random Forest\n",
    "    gb_oof_train_iq_m, gb_oof_test_iq_m = get_OutOfFoldPredictions( gb_iq, x_train_iq, y_train_iq_m, x_test_iq ) # Gradient Boost\n",
    "    x_train_RFE_iq = x_train_iq[col_RFE_iq]\n",
    "    x_test_RFE_iq  = x_test_iq[col_RFE_iq]\n",
    "    nb_oof_train_iq_m, nb_oof_test_iq_m = get_OutOfFoldPredictions( nb_iq, x_train_RFE_iq, y_train_iq_m, x_test_RFE_iq )  # Negative Binomial\n",
    "    x_train_lr_iq = x_train_iq[col_FFS_iq]\n",
    "    x_test_lr_iq  = x_test_iq[col_FFS_iq]\n",
    "    lr_oof_train_iq_m, lr_oof_test_iq_m = get_OutOfFoldPredictions( lr_iq, x_train_lr_iq, y_train_iq_m, x_test_lr_iq )  # Linear Regression\n",
    "    rf_oof_train_iq = np.append( rf_oof_train_iq, rf_oof_train_iq_m)\n",
    "    rf_oof_test_iq  = np.append( rf_oof_test_iq , rf_oof_test_iq_m )\n",
    "    gb_oof_train_iq = np.append( gb_oof_train_iq, gb_oof_train_iq_m)\n",
    "    gb_oof_test_iq  = np.append( gb_oof_test_iq , gb_oof_test_iq_m )\n",
    "    nb_oof_train_iq = np.append( nb_oof_train_iq, nb_oof_train_iq_m)\n",
    "    nb_oof_test_iq  = np.append( nb_oof_test_iq , nb_oof_test_iq_m )\n",
    "    lr_oof_train_iq = np.append( lr_oof_train_iq, lr_oof_train_iq_m)\n",
    "    lr_oof_test_iq  = np.append( lr_oof_test_iq , lr_oof_test_iq_m)\n",
    "    y_train_iq = np.append( y_train_iq , y_train_iq_m )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1201ecc-b07d-f8a1-0870-b8d78c89ebc0",
    "_uuid": "bbb76d189e8d03921caaacfa9545cef894348c7d"
   },
   "source": [
    "# Second-Level Predictions from the First-level Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b901750-ccdd-38ca-d8ea-1c361121ec4f",
    "_uuid": "fed132782b73dda8d265065867e7f57c0aed7f50"
   },
   "source": [
    "**First-level output as new features**\n",
    "\n",
    "Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next regression. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier regressors and we train the next regression on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_cell_guid": "7330a71c-0b71-87c2-1f4d-dd0f6d6fa586",
    "_uuid": "a5945e93337b87a1a8ee5580856768bbb14c07cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RandomForest  GradientBoost  NegativeBinomial  LinearRegression\n",
      "0     34.052273      29.743795         56.734418         29.754986\n",
      "1     30.811364      25.842989         66.486865         36.788830\n",
      "2     31.663636      26.968149         45.285490         37.443580\n",
      "3     30.818182      25.467220         31.016681         23.613596\n",
      "4     31.361364      27.874380         25.114499         25.279974\n",
      "   RandomForest  GradientBoost  NegativeBinomial  LinearRegression\n",
      "0     18.545455      15.255092          7.799189         41.623884\n",
      "1     18.136364      15.255092         10.697936         45.381529\n",
      "2     20.181818      18.691101          7.435928         41.298359\n",
      "3     27.863636      21.634377          8.165964         41.541228\n",
      "4     22.090909      19.177652         31.420405         35.738764\n",
      "   RandomForest  GradientBoost  NegativeBinomial  LinearRegression\n",
      "0     27.017500      25.355682          6.232939         26.396776\n",
      "1     25.692500      25.339326         12.277291         27.304323\n",
      "2     25.155455      27.049669         22.005672         37.222810\n",
      "3     24.325909      28.135031         18.682367         45.427553\n",
      "4     23.302273      22.897555         27.592336         39.880514\n",
      "   RandomForest  GradientBoost  NegativeBinomial  LinearRegression\n",
      "0     15.536364      14.658934          7.223723         10.328607\n",
      "1     14.990909      13.501655         10.951262         14.134330\n",
      "2     13.306818      13.047069          7.451994         14.328779\n",
      "3     13.040909      13.213476          7.178722         15.303244\n",
      "4     11.531818       8.124884          6.815843         15.585721\n"
     ]
    }
   ],
   "source": [
    "base_predictions_train_sj = pd.DataFrame({'RandomForest':     rf_oof_train_sj.ravel(),\n",
    "                                          'GradientBoost':    gb_oof_train_sj.ravel(),\n",
    "                                          'NegativeBinomial': nb_oof_train_sj.ravel(),\n",
    "                                          'LinearRegression': lr_oof_train_sj.ravel(),\n",
    "                                         })\n",
    "print( base_predictions_train_sj.head() )\n",
    "base_predictions_train_iq = pd.DataFrame({'RandomForest':     rf_oof_train_iq.ravel(),\n",
    "                                          'GradientBoost':    gb_oof_train_iq.ravel(),\n",
    "                                          'NegativeBinomial': nb_oof_train_iq.ravel(),\n",
    "                                          'LinearRegression': lr_oof_train_iq.ravel(),\n",
    "                                         })\n",
    "print( base_predictions_train_iq.head())\n",
    "\n",
    "base_predictions_test_sj = pd.DataFrame({ 'RandomForest':     rf_oof_test_sj.ravel(),\n",
    "                                          'GradientBoost':    gb_oof_test_sj.ravel(),\n",
    "                                          'NegativeBinomial': nb_oof_test_sj.ravel(),\n",
    "                                          'LinearRegression': lr_oof_test_sj.ravel(),\n",
    "                                         })\n",
    "print( base_predictions_test_sj.head() )\n",
    "base_predictions_test_iq = pd.DataFrame({ 'RandomForest':     rf_oof_test_iq.ravel(),\n",
    "                                          'GradientBoost':    gb_oof_test_iq.ravel(),\n",
    "                                          'NegativeBinomial': nb_oof_test_iq.ravel(),\n",
    "                                          'LinearRegression': lr_oof_test_iq.ravel(),\n",
    "                                         })\n",
    "print( base_predictions_test_iq.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f69c11db-d84e-8536-4c7e-382fbe67483e",
    "_uuid": "2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200"
   },
   "source": [
    "**Correlation Heatmap of the Second Level Training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "4cf590ee-133f-6487-cf5a-53f346893d1c",
    "_uuid": "9714ecaedf7385c5b8ad346ab909215eb9f2abc6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": true,
         "type": "heatmap",
         "uid": "da74ac90-aa9b-4eb0-8493-278a2169665d",
         "x": [
          "RandomForest",
          "GradientBoost",
          "NegativeBinomial",
          "LinearRegression"
         ],
         "y": [
          "RandomForest",
          "GradientBoost",
          "NegativeBinomial",
          "LinearRegression"
         ],
         "z": [
          [
           1,
           0.805030633687429,
           0.3915975785544312,
           0.6785872485707476
          ],
          [
           0.805030633687429,
           1,
           0.4153077111649227,
           0.6666918620675782
          ],
          [
           0.3915975785544312,
           0.4153077111649227,
           1,
           0.4944459597357996
          ],
          [
           0.6785872485707476,
           0.6666918620675782,
           0.4944459597357996,
           1
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"ccc860ee-fd78-459b-b13c-6d8b8bb6bcea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"ccc860ee-fd78-459b-b13c-6d8b8bb6bcea\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'ccc860ee-fd78-459b-b13c-6d8b8bb6bcea',\n",
       "                        [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": true, \"type\": \"heatmap\", \"uid\": \"da74ac90-aa9b-4eb0-8493-278a2169665d\", \"x\": [\"RandomForest\", \"GradientBoost\", \"NegativeBinomial\", \"LinearRegression\"], \"y\": [\"RandomForest\", \"GradientBoost\", \"NegativeBinomial\", \"LinearRegression\"], \"z\": [[1.0, 0.805030633687429, 0.3915975785544312, 0.6785872485707476], [0.805030633687429, 1.0, 0.4153077111649227, 0.6666918620675782], [0.3915975785544312, 0.4153077111649227, 1.0, 0.4944459597357996], [0.6785872485707476, 0.6666918620675782, 0.4944459597357996, 1.0]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ccc860ee-fd78-459b-b13c-6d8b8bb6bcea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [ go.Heatmap( z = base_predictions_train_sj.astype( float ).corr().values,\n",
    "                     x = base_predictions_train_sj.columns.values,\n",
    "                     y = base_predictions_train_sj.columns.values,\n",
    "                     colorscale = 'Viridis',\n",
    "                     showscale = True,\n",
    "                     reversescale = True )]\n",
    "py.iplot( data, filename = 'labelled-heatmap' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": true,
         "type": "heatmap",
         "uid": "cb4874db-e4e2-48a9-8146-ef71b2e21406",
         "x": [
          "RandomForest",
          "GradientBoost",
          "NegativeBinomial",
          "LinearRegression"
         ],
         "y": [
          "RandomForest",
          "GradientBoost",
          "NegativeBinomial",
          "LinearRegression"
         ],
         "z": [
          [
           1,
           0.8268855575333961,
           0.3187817161860674,
           0.4205056043254802
          ],
          [
           0.8268855575333961,
           1,
           0.28628005330762213,
           0.44816485011131396
          ],
          [
           0.3187817161860674,
           0.28628005330762213,
           1,
           0.3043616633899231
          ],
          [
           0.4205056043254802,
           0.44816485011131396,
           0.3043616633899231,
           1
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"03cff44b-59d0-41f6-8638-a3184b88f226\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"03cff44b-59d0-41f6-8638-a3184b88f226\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '03cff44b-59d0-41f6-8638-a3184b88f226',\n",
       "                        [{\"colorscale\": \"Viridis\", \"reversescale\": true, \"showscale\": true, \"type\": \"heatmap\", \"uid\": \"cb4874db-e4e2-48a9-8146-ef71b2e21406\", \"x\": [\"RandomForest\", \"GradientBoost\", \"NegativeBinomial\", \"LinearRegression\"], \"y\": [\"RandomForest\", \"GradientBoost\", \"NegativeBinomial\", \"LinearRegression\"], \"z\": [[1.0, 0.8268855575333961, 0.3187817161860674, 0.4205056043254802], [0.8268855575333961, 1.0, 0.28628005330762213, 0.44816485011131396], [0.3187817161860674, 0.28628005330762213, 1.0, 0.3043616633899231], [0.4205056043254802, 0.44816485011131396, 0.3043616633899231, 1.0]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('03cff44b-59d0-41f6-8638-a3184b88f226');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [ go.Heatmap( z = base_predictions_train_iq.astype( float ).corr().values,\n",
    "                     x = base_predictions_train_iq.columns.values,\n",
    "                     y = base_predictions_train_iq.columns.values,\n",
    "                     colorscale = 'Viridis',\n",
    "                     showscale = True,\n",
    "                     reversescale = True )]\n",
    "py.iplot( data, filename = 'labelled-heatmap' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sec_sj = np.array( base_predictions_train_sj )\n",
    "x_test_sec_sj  = np.array( base_predictions_test_sj )\n",
    "x_train_sec_iq = np.array( base_predictions_train_iq )\n",
    "x_test_sec_iq  = np.array( base_predictions_test_iq )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a02a94ab-3c9c-a824-7168-e964c5a0f5d5",
    "_uuid": "65727ae393d3f7118215fde76e4fd5a9d0e9dd6c"
   },
   "source": [
    "Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "628a03ea-933c-7075-a589-0ff7af237dfd",
    "_uuid": "dc4a32e9a8e7c9e611124cba676e5d28240b38be"
   },
   "source": [
    "### Second level learning model via XGBoost\n",
    "\n",
    "Here we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n",
    "\n",
    "  [1]: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "Anyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:02:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 1440 out of 1440 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame( x_train_sec_sj )\n",
    "y = y_train_sj\n",
    "parameters_for_testing = { 'learning_rate':[0.01],#0.1,0.01\n",
    "                           'max_depth':[3,4,5],#,6\n",
    "                           'n_estimators':[150,200,300,250],#100,200,500,1000\n",
    "                           'gamma':[0],#0,0.01\n",
    "                           'min_child_weight':[5,8,12,15],\n",
    "                           'colsample_bytree':[0.5,0.4,0.6],\n",
    "                           'reg_alpha':[0.01],\n",
    "                           'reg_lambda':[1e-5,1e-4],#0.075\n",
    "                           'subsample':[0.95]#,1e-5\n",
    "                         }\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "gs_xg_sj = GridSearchCV( estimator = xgb_model, param_grid = parameters_for_testing,\n",
    "                      n_jobs=4,iid=False, verbose=1, scoring ='neg_mean_squared_error',\n",
    "                      cv= 5).fit( X, y, eval_metric='rmse' )\n",
    "Y_sj_pred = gs_xg_sj.best_estimator_.predict( pd.DataFrame( x_test_sec_sj, columns = X.columns ) ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame( x_train_sec_iq )\n",
    "y = y_train_iq\n",
    "parameters_for_testing = { 'learning_rate':[0.1,0.01],\n",
    "                           'max_depth':[2,3,4,5],\n",
    "                           'n_estimators':[150,200,250],#100,200,300,500\n",
    "                           'gamma':[0],#,0.01\n",
    "                           'min_child_weight':[12,18,15,20],#5,8,\n",
    "                           'colsample_bytree':[0.4,0.6],\n",
    "                           'reg_alpha':[0.01],\n",
    "                           'reg_lambda':[1e-5,1e-4,0.075],\n",
    "                           'subsample':[0.95]#,1e-5,1e-4\n",
    "                         }\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "gs_xg_iq = GridSearchCV( estimator = xgb_model, param_grid = parameters_for_testing,\n",
    "                      n_jobs=4,iid=False, verbose=1, scoring ='neg_mean_squared_error',\n",
    "                      cv= 5).fit( X, y, eval_metric='rmse' )\n",
    "\n",
    "Y_iq_pred = gs_xg_iq.best_estimator_.predict( pd.DataFrame( x_test_sec_iq, columns = X.columns )).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest as Last model\n",
    "# X = pd.DataFrame( x_train_sec_sj )\n",
    "# y = y_train_sj\n",
    "# param_grid = { \n",
    "#     \"n_estimators\"      : [160,220,150],\n",
    "#     \"max_features\"      : [\"sqrt\"],\n",
    "#     \"min_samples_split\" : [10,12,18],\n",
    "#     \"bootstrap\"         : [True],\n",
    "#     \"max_depth\"         : [2,4,5,8]\n",
    "#     }\n",
    "# estimator = RandomForestRegressor( criterion='mae', oob_score=True)\n",
    "# rf_est = GridSearchCV(estimator, param_grid,\n",
    "#                       n_jobs=-1, cv=5, verbose=0,\n",
    "#                       iid=True ).fit( X, y)\n",
    "# Y_sj_pred = rf_est.best_estimator_.predict(pd.DataFrame( x_test_sec_sj, columns = X.columns )).astype(int)\n",
    "# \n",
    "# X = pd.DataFrame( x_train_sec_iq )\n",
    "# y = y_train_iq\n",
    "# param_grid = { \n",
    "#     \"n_estimators\"      : [160,220,100],\n",
    "#     \"max_features\"      : [\"sqrt\"],\n",
    "#     \"min_samples_split\" : [10,16,20],\n",
    "#     \"bootstrap\"         : [True],\n",
    "#     \"max_depth\"         : [2,4,5,8]\n",
    "#     }\n",
    "# estimator = RandomForestRegressor( criterion='mae', oob_score=True)\n",
    "# rf_est = GridSearchCV(estimator, param_grid,\n",
    "#                       n_jobs=-1, cv=5, verbose=0, iid=True).fit( X, y)\n",
    "# Y_iq_pred = rf_est.best_estimator_.predict(pd.DataFrame( x_test_sec_iq, columns = X.columns )).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_sj = pd.DataFrame( dengue_test_sj, columns = ['city'])\n",
    "submission_iq = pd.DataFrame( dengue_test_iq, columns = ['city'])\n",
    "submission_sj.reset_index( inplace = True)\n",
    "submission_sj['year'] = submission_sj['week_start_date'].dt.year\n",
    "submission_sj['weekofyear'] = submission_sj['week_start_date'].dt.weekofyear\n",
    "submission_iq.reset_index( inplace = True)\n",
    "submission_iq['year'] = submission_iq['week_start_date'].dt.year\n",
    "submission_iq['weekofyear'] = submission_iq['week_start_date'].dt.weekofyear\n",
    "submission = submission_sj.append( submission_iq )\n",
    "submission.drop( columns = ['week_start_date'], inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_pred_sj = pd.DataFrame()\n",
    "for month in range( 1, 13 ):\n",
    "    test_filename  = ( './../datasets/test_sj_month_' + str( month ) + '.pkl' )\n",
    "    dengue_test_sj_month = pd.read_pickle( test_filename )\n",
    "    submit_pred_sj_month = pd.DataFrame( dengue_test_sj_month, columns = ['city', 'week_start_date'])\n",
    "    submit_pred_sj = submit_pred_sj.append( submit_pred_sj_month )\n",
    "submit_pred_sj.drop( columns=['week_start_date'], inplace = True )\n",
    "submit_pred_sj.reset_index( inplace = True )\n",
    "submit_pred_sj['year'] = submit_pred_sj['week_start_date'].dt.year\n",
    "submit_pred_sj['weekofyear'] = submit_pred_sj['week_start_date'].dt.weekofyear\n",
    "submit_pred_sj.drop( columns = ['week_start_date'], inplace = True )\n",
    "submit_pred_sj['total_cases'] = Y_sj_pred\n",
    "\n",
    "submit_pred_iq = pd.DataFrame()\n",
    "for month in range( 1, 13 ):\n",
    "    test_filename  = ( './../datasets/test_iq_month_' + str( month ) + '.pkl' )\n",
    "    dengue_test_iq_month = pd.read_pickle( test_filename )\n",
    "    submit_pred_iq_month = pd.DataFrame( dengue_test_iq_month, columns = ['city', 'week_start_date'])\n",
    "    submit_pred_iq = submit_pred_iq.append( submit_pred_iq_month )\n",
    "submit_pred_iq = pd.DataFrame( dengue_test_iq, columns = ['city', 'week_start_date'])\n",
    "submit_pred_iq.drop( columns=['week_start_date'], inplace = True )\n",
    "submit_pred_iq.reset_index( inplace = True )\n",
    "submit_pred_iq['year'] = submit_pred_iq['week_start_date'].dt.year\n",
    "submit_pred_iq['weekofyear'] = submit_pred_iq['week_start_date'].dt.weekofyear\n",
    "submit_pred_iq.drop( columns = ['week_start_date'], inplace = True )\n",
    "submit_pred_iq['total_cases'] = Y_iq_pred\n",
    "\n",
    "submit = submit_pred_sj.append( submit_pred_iq, sort=True )\n",
    "test = pd.merge( submission, submit, on =['city','year','weekofyear'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"data/ensemble_stacking_rf_xgb_nb_linear_monthwise.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
